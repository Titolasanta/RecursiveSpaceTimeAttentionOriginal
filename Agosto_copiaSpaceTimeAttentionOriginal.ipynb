{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "boYvYfflu8Ln"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M37P2hUmvEMa",
        "outputId": "48a5b52d-5914-4476-cf15-0b13df5f0c2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement pickle (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for pickle\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "#torchdata==0.3.0 torchtext==0.12 spacy==3.2 altair datasets\n",
        "!pip install -q  GPUtil pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qz85VpeuvKGD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t85BhuUfvKLy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "class Configs:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "configs = Configs()\n",
        "\n",
        "# trainer relatedconfigs.n_cpu = 0\n",
        "configs.device = torch.device('cuda:0')\n",
        "configs.batch_size_test = 100\n",
        "configs.batch_size = 8\n",
        "#configs.lr = 0.001\n",
        "configs.weight_decay = 0.0000\n",
        "configs.display_interval = 120\n",
        "configs.num_epochs = 100\n",
        "configs.early_stopping = True\n",
        "configs.patience = 5#3\n",
        "configs.gradient_clipping = True # false\n",
        "configs.clipping_threshold = 1.\n",
        "\n",
        "# lr warmup\n",
        "configs.warmup = 3000\n",
        "\n",
        "# data related\n",
        "configs.input_dim = 1 # 4\n",
        "configs.output_dim = 1\n",
        "\n",
        "configs.input_length = 12\n",
        "configs.output_length = 26\n",
        "\n",
        "configs.input_gap = 1\n",
        "configs.pred_shift = 24\n",
        "\n",
        "# model\n",
        "configs.d_model = 256\n",
        "configs.patch_size = (2, 3)\n",
        "configs.emb_spatial_size = 12*16\n",
        "configs.nheads = 4\n",
        "configs.dim_feedforward = 512\n",
        "configs.dropout = 0.05\n",
        "configs.num_encoder_layers = 2#3\n",
        "configs.num_decoder_layers = 2#3\n",
        "\n",
        "configs.ssr_decay_rate = 3.e-4 #3.e-4  ssr_decay_rate aumentar empeora a negativo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#trainer = Trainer(configs)\n",
        "#total_params = sum(p.numel() for p in trainer.network.parameters() if p.requires_grad)\n",
        "#print(f\"Total trainable parameters: {total_params}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "EbPPvnmYR5U5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhB1Fh20vCmL"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Author: Jiacheng WU\n",
        "Modified by Tito Lasanta\n",
        "The idea is mainly from Transformer (https://arxiv.org/abs/1706.03762),\n",
        "and TimeSformer (https://arxiv.org/abs/2102.05095).\n",
        "We novelly modified the TimeSformer for spatio-temporal prediction.\n",
        "\"\"\"\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import copy\n",
        "\n",
        "\n",
        "class SpaceTimeTransformer(nn.Module):\n",
        "    def __init__(self, configs):\n",
        "        super().__init__()\n",
        "        self.configs = configs\n",
        "        d_model = configs.d_model\n",
        "        self.device = configs.device\n",
        "        self.input_dim = configs.input_dim * configs.patch_size[0] * configs.patch_size[1]\n",
        "        self.src_emb = input_embedding(self.input_dim, d_model, configs.emb_spatial_size,\n",
        "                                       configs.input_length, self.device)\n",
        "        self.tgt_emb = input_embedding(self.input_dim, d_model, configs.emb_spatial_size,\n",
        "                                       configs.output_length, self.device)\n",
        "\n",
        "        encoder_layer = EncoderLayer(d_model, configs.nheads, configs.dim_feedforward, configs.dropout)\n",
        "        decoder_layer = DecoderLayer(d_model, configs.nheads, configs.dim_feedforward, configs.dropout)\n",
        "        self.encoder = Encoder(encoder_layer, num_layers=configs.num_encoder_layers)\n",
        "        self.decoder = Decoder(decoder_layer, num_layers=configs.num_decoder_layers)\n",
        "        self.linear_output = nn.Linear(d_model, self.input_dim)\n",
        "\n",
        "    def forward(self, src, tgt, src_mask=None, memory_mask=None,\n",
        "                train=True, ssr_ratio=0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            src: (N, T_src, C, H, W)\n",
        "            tgt: (N, T_tgt, C, H, W), T_tgt is 1 during test\n",
        "            src_mask: (T_src, T_src)\n",
        "            tgt_mask: (T_tgt, T_tgt)\n",
        "            memory_mask: (T_tgt, T_src)\n",
        "        Returns:\n",
        "            sst_pred: (N, T_tgt, H, W)\n",
        "            nino_pred: (N, 24)\n",
        "        \"\"\"\n",
        "        memory = self.encode(src, src_mask)\n",
        "        if train:\n",
        "            with torch.no_grad():\n",
        "                # tgt = torch.cat([src[:, -1:], tgt[:, :-1]], dim=1)  # (N, T_tgt, C, H, W)\n",
        "                tgt_mask = self.generate_square_subsequent_mask(tgt.size(1))\n",
        "                sst_pred = self.decode(torch.cat([src[:, -1:], tgt[:, :-1]], dim=1),\n",
        "                                       memory, tgt_mask, memory_mask)  # (N, T_tgt, C, H, W)\n",
        "\n",
        "            if ssr_ratio > 1e-6:\n",
        "                teacher_forcing_mask = torch.bernoulli(ssr_ratio *\n",
        "                        torch.ones(tgt.size(0), tgt.size(1) - 1, 1, 1, 1)).to(self.device)\n",
        "            else:\n",
        "                teacher_forcing_mask = 0\n",
        "            tgt = teacher_forcing_mask * tgt[:, :-1] + (1 - teacher_forcing_mask) * sst_pred[:, :-1]\n",
        "            tgt = torch.cat([src[:, -1:], tgt], dim=1)\n",
        "            sst_pred = self.decode(tgt, memory, tgt_mask, memory_mask)\n",
        "        else:\n",
        "            if tgt is None:\n",
        "                tgt = src[:, -1:]  # use last src as the input during test\n",
        "            else:\n",
        "                assert tgt.size(1) == 1\n",
        "            for t in range(self.configs.output_length):\n",
        "                tgt_mask = self.generate_square_subsequent_mask(tgt.size(1))\n",
        "                sst_pred = self.decode(tgt, memory, tgt_mask, memory_mask)\n",
        "                tgt = torch.cat([tgt, sst_pred[:, -1:]], dim=1)\n",
        "\n",
        "        sst_pred = sst_pred[:, :, 0]  # (N, T_tgt, H, W)\n",
        "        nino_pred = sst_pred[:, :, 10:13, 19:30].mean(dim=[2, 3])  # (N, 26)\n",
        "        nino_pred = nino_pred.unfold(dimension=1, size=3, step=1).mean(dim=2)  # (N, 24)\n",
        "\n",
        "        return sst_pred, nino_pred\n",
        "\n",
        "    def encode(self, src, src_mask):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            src: (N, T_src, C, H, W)\n",
        "            src_mask: (T_src, T_src)\n",
        "        Returns:\n",
        "            memory: (N, S, T_src, D)\n",
        "        \"\"\"\n",
        "        T = src.size(1)\n",
        "        src = unfold_StackOverChannel(src, self.configs.patch_size)  # (N, T_src, C_, H_, W_)\n",
        "        src = src.reshape(src.size(0), T, self.input_dim, -1).permute(0, 3, 1, 2)  # (N, S, T_src, C_)\n",
        "        src = self.src_emb(src)  # (N, S, T_src, D)\n",
        "        memory = self.encoder(src, src_mask)  # (N, S, T_src, D)\n",
        "        return memory\n",
        "\n",
        "    def decode(self, tgt, memory, tgt_mask, memory_mask):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            tgt: (N, T_tgt, C, H, W)\n",
        "            memory: (N, S, T_src, D)\n",
        "            tgt_mask: (T_tgt, T_tgt)\n",
        "            memory_mask: (T_tgt, T_src)\n",
        "        Returns:\n",
        "            (N, T_tgt, C, H, W)\n",
        "        \"\"\"\n",
        "        H, W = tgt.size()[-2:]\n",
        "        T = tgt.size(1)\n",
        "        tgt = unfold_StackOverChannel(tgt, self.configs.patch_size)  # (N, T_tgt, C_, H_, W_)\n",
        "        tgt = tgt.reshape(tgt.size(0), T, self.input_dim, -1).permute(0, 3, 1, 2)  # (N, S, T_tgt, C_)\n",
        "        tgt = self.tgt_emb(tgt)  # (N, S, T_tgt, D)\n",
        "        output = self.decoder(tgt, memory, tgt_mask, memory_mask)\n",
        "        output = self.linear_output(output).permute(0, 2, 3, 1)  # (N, T_tgt, C_, S)\n",
        "\n",
        "        # (N, T_tgt, C_, H_, W_)\n",
        "        output = output.reshape(tgt.size(0), T, self.input_dim,\n",
        "                                H // self.configs.patch_size[0], W // self.configs.patch_size[1])\n",
        "        # (N, T_tgt, C, H, W)\n",
        "        output = fold_tensor(output, output_size=(H, W), kernel_size=self.configs.patch_size)\n",
        "        return output\n",
        "\n",
        "    def generate_square_subsequent_mask(self, sz: int):\n",
        "        \"\"\"\n",
        "        Generate a square mask for the sequence. The masked positions are filled with float('-inf')\n",
        "        \"\"\"\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 0).T\n",
        "        return mask.to(self.configs.device)\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, encoder_layer, num_layers):\n",
        "        super().__init__()\n",
        "        #self.layers = nn.ModuleList([copy.deepcopy(encoder_layer) for _ in range(num_layers)])\n",
        "        self.layers = nn.ModuleList([encoder_layer for _ in range(num_layers)])\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, decoder_layer, num_layers):\n",
        "        super().__init__()\n",
        "        #self.layers = nn.ModuleList([copy.deepcopy(decoder_layer) for _ in range(num_layers)])\n",
        "        self.layers = nn.ModuleList([decoder_layer for _ in range(num_layers)])\n",
        "\n",
        "    def forward(self, x, memory, tgt_mask, memory_mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, memory, tgt_mask, memory_mask)\n",
        "        return x\n",
        "\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, nheads, dim_feedforward, dropout):\n",
        "        super().__init__()\n",
        "        self.sublayer = clones(SublayerConnection(d_model, dropout), 2)\n",
        "        self.time_attn = MultiHeadedAttention(d_model, nheads, TimeAttention, dropout)\n",
        "        self.space_attn = MultiHeadedAttention(d_model, nheads, SpaceAttention, dropout)\n",
        "\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(d_model, dim_feedforward),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(dim_feedforward, d_model)\n",
        "            )\n",
        "\n",
        "    def divided_space_time_attn(self, query, key, value, mask):\n",
        "        \"\"\"\n",
        "        Apply space and time attention sequentially\n",
        "        Args:\n",
        "            query (N, S, T, D)\n",
        "            key (N, S, T, D)\n",
        "            value (N, S, T, D)\n",
        "        Returns:\n",
        "            (N, S, T, D)\n",
        "        \"\"\"\n",
        "        m = self.time_attn(query, key, value, mask)\n",
        "        return self.space_attn(m, m, m, mask)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x = self.sublayer[0](x, lambda x: self.divided_space_time_attn(x, x, x, mask))\n",
        "        return self.sublayer[1](x, self.feed_forward)\n",
        "\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, nheads, dim_feedforward, dropout):\n",
        "        super().__init__()\n",
        "        self.sublayer = clones(SublayerConnection(d_model, dropout), 3)\n",
        "        self.encoder_attn = MultiHeadedAttention(d_model, nheads, TimeAttention, dropout)\n",
        "        self.time_attn = MultiHeadedAttention(d_model, nheads, TimeAttention, dropout)\n",
        "        self.space_attn = MultiHeadedAttention(d_model, nheads, SpaceAttention, dropout)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(d_model, dim_feedforward),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(dim_feedforward, d_model)\n",
        "            )\n",
        "\n",
        "    def divided_space_time_attn(self, query, key, value, mask=None):\n",
        "        \"\"\"\n",
        "        Apply space and time attention sequentially\n",
        "        Args:\n",
        "            query (N, S, T_q, D)\n",
        "            key (N, S, T, D)\n",
        "            value (N, S, T, D)\n",
        "        Returns:\n",
        "            (N, S, T_q, D)\n",
        "        \"\"\"\n",
        "        m = self.time_attn(query, key, value, mask)\n",
        "        return self.space_attn(m, m, m, mask)\n",
        "    '''\n",
        "    def joint_space_time_attention(self, query, key, value, mask):\n",
        "        \"\"\"\n",
        "        not applicable in the decoder when the memory is attened,\n",
        "        as the time length of query is not the same as T\n",
        "        Args:\n",
        "            query (N, S, T_q, D) -> (N, 1, S*T_q, D)\n",
        "            key (N, S, T, D)\n",
        "            value (N, S, T, D)\n",
        "        Returns:\n",
        "            (N, S, T_q, D)\n",
        "        \"\"\"\n",
        "        nbatches, nspace, ntime = query.size()[:3]\n",
        "        query = query.flatten(1, 2).unsqueeze(1)\n",
        "        key = key.flatten(1, 2).unsqueeze(1)\n",
        "        value = value.flatten(1, 2).unsqueeze(1)\n",
        "        output = self.encoder_attn(query, key, value, mask)  # (N, 1, S*T_q, D)\n",
        "        output = output.reshape(nbatches, nspace, ntime, -1)  # (N, S, T_q, D)\n",
        "        return output\n",
        "    '''\n",
        "    def forward(self, x, memory, tgt_mask, memory_mask):\n",
        "        x = self.sublayer[0](x, lambda x: self.divided_space_time_attn(x, x, x, tgt_mask))\n",
        "        x = self.sublayer[1](x, lambda x: self.encoder_attn(x, memory, memory, memory_mask))\n",
        "        return self.sublayer[2](x, self.feed_forward)\n",
        "\n",
        "\n",
        "def unfold_StackOverChannel(img, kernel_size):\n",
        "    \"\"\"\n",
        "    divide the original image to patches, then stack the grids in each patch along the channels\n",
        "    Args:\n",
        "        img (N, *, C, H, W): the last two dimensions must be the spatial dimension\n",
        "        kernel_size: tuple of length 2\n",
        "    Returns:\n",
        "        output (N, *, C*H_k*N_k, H_output, W_output)\n",
        "    \"\"\"\n",
        "    n_dim = len(img.size())\n",
        "    assert n_dim == 4 or n_dim == 5\n",
        "\n",
        "    pt = img.unfold(-2, size=kernel_size[0], step=kernel_size[0])\n",
        "\n",
        "    pt = pt.unfold(-2, size=kernel_size[1], step=kernel_size[1]).flatten(-2)  # (N, *, C, n0, n1, k0*k1)\n",
        "    if n_dim == 4:  # (N, C, H, W)\n",
        "        pt = pt.permute(0, 1, 4, 2, 3).flatten(1, 2)\n",
        "    elif n_dim == 5:  # (N, T, C, H, W)\n",
        "        pt = pt.permute(0, 1, 2, 5, 3, 4).flatten(2, 3)\n",
        "    assert pt.size(-3) == img.size(-3) * kernel_size[0] * kernel_size[1]\n",
        "    return pt\n",
        "\n",
        "\n",
        "def fold_tensor(tensor, output_size, kernel_size):\n",
        "    \"\"\"\n",
        "    reconstruct the image from its non-overlapping patches\n",
        "    Args:\n",
        "        input tensor of size (N, *, C*k_h*k_w, n_h, n_w)\n",
        "        output_size of size(H, W), the size of the original image to be reconstructed\n",
        "        kernel_size: (k_h, k_w)\n",
        "        stride is usually equal to kernel_size for non-overlapping sliding window\n",
        "    Returns:\n",
        "        (N, *, C, H=n_h*k_h, W=n_w*k_w)\n",
        "    \"\"\"\n",
        "    tensor = tensor.float()\n",
        "    n_dim = len(tensor.size())\n",
        "    assert n_dim == 4 or n_dim == 5\n",
        "    f = tensor.flatten(0, 1) if n_dim == 5 else tensor\n",
        "    folded = F.fold(f.flatten(-2), output_size=output_size, kernel_size=kernel_size, stride=kernel_size)\n",
        "    if n_dim == 5:\n",
        "        folded = folded.reshape(tensor.size(0), tensor.size(1), *folded.size()[1:])\n",
        "    return folded\n",
        "\n",
        "\n",
        "def clones(module, N):\n",
        "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
        "\n",
        "\n",
        "class SublayerConnection(nn.Module):\n",
        "    def __init__(self, size, dropout):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        return self.norm(x + self.dropout(sublayer(x)))\n",
        "\n",
        "\n",
        "class input_embedding(nn.Module):\n",
        "    def __init__(self, input_dim, d_model, emb_spatial_size, max_len, device):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
        "                             -(np.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        self.pe_time = pe[None, None].to(device)  # (1, 1, T, D)\n",
        "        self.spatial_pos = torch.arange(emb_spatial_size)[None, :, None].to(device)\n",
        "        self.emb_space = nn.Embedding(emb_spatial_size, d_model)\n",
        "        self.linear = nn.Linear(input_dim, d_model)\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Add temporal encoding and learnable spatial embedding to the input (after patch)\n",
        "        Args:\n",
        "            input x of size (N, S, T, in_channels)\n",
        "        Returns:\n",
        "            embedded input (N, S, T, D)\n",
        "        \"\"\"\n",
        "        assert len(x.size()) == 4\n",
        "        embedded_space = self.emb_space(self.spatial_pos)  # (1, S, 1, D)\n",
        "        x = self.linear(x) + self.pe_time[:, :, :x.size(2)] + embedded_space  # (N, S, T, D)\n",
        "        return self.norm(x)\n",
        "\n",
        "\n",
        "def TimeAttention(query, key, value, mask=None, dropout=None):\n",
        "    \"\"\"\n",
        "    attention over the time axis\n",
        "    Args:\n",
        "        query, key, value: linearly-transformed query, key, value (N, h, S, T, D)\n",
        "        mask: of size (T (query), T (key)) specifying locations (which key) the query can and cannot attend to\n",
        "    \"\"\"\n",
        "    d_k = query.size(-1)\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) / np.sqrt(d_k)  # (N, h, S, T, T)\n",
        "    if mask is not None:\n",
        "        assert mask.dtype == torch.bool\n",
        "        assert len(mask.size()) == 2\n",
        "        scores = scores.masked_fill(mask[None, None, None], float(\"-inf\"))\n",
        "    p_attn = F.softmax(scores, dim=-1)\n",
        "    if dropout is not None:\n",
        "        p_attn = dropout(p_attn)\n",
        "    return torch.matmul(p_attn, value)  # (N, h, S, T, D)\n",
        "\n",
        "\n",
        "def SpaceAttention(query, key, value, mask=None, dropout=None):\n",
        "    \"\"\"\n",
        "    attention over the two space axes\n",
        "    Args:\n",
        "        query, key, value: linearly-transformed query, key, value (N, h, S, T, D)\n",
        "        mask: None (space attention does not need mask), this argument is intentionally set for consistency\n",
        "    \"\"\"\n",
        "    d_k = query.size(-1)\n",
        "    query = query.transpose(2, 3)  # (N, h, T, S, D)\n",
        "    key = key.transpose(2, 3)  # (N, h, T, S, D)\n",
        "    value = value.transpose(2, 3)  # (N, h, T, S, D)\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) / np.sqrt(d_k)  # (N, h, T, S, S)\n",
        "    p_attn = F.softmax(scores, dim=-1)\n",
        "    if dropout is not None:\n",
        "        p_attn = dropout(p_attn)\n",
        "    return torch.matmul(p_attn, value).transpose(2, 3)  # (N, h, S, T_q, D)\n",
        "\n",
        "\n",
        "class MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self, d_model, nheads, attn, dropout):\n",
        "        super().__init__()\n",
        "        assert d_model % nheads == 0\n",
        "        self.d_k = d_model // nheads\n",
        "        self.nheads = nheads\n",
        "        self.linears = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(4)])\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.attn = attn\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        \"\"\"\n",
        "        Transform the query, key, value into different heads, then apply the attention in parallel\n",
        "        Args:\n",
        "            query, key, value: size (N, S, T, D)\n",
        "        Returns:\n",
        "            (N, S, T, D)\n",
        "        \"\"\"\n",
        "        nbatches = query.size(0)\n",
        "        nspace = query.size(1)\n",
        "        ntime = query.size(2)\n",
        "        # (N, h, S, T, d_k)\n",
        "        query, key, value = \\\n",
        "            [l(x).view(x.size(0), x.size(1), x.size(2), self.nheads, self.d_k).permute(0, 3, 1, 2, 4)\n",
        "             for l, x in zip(self.linears, (query, key, value))]\n",
        "\n",
        "        # (N, h, S, T, d_k)\n",
        "        x = self.attn(query, key, value, mask=mask, dropout=self.dropout)\n",
        "\n",
        "        # (N, S, T, D)\n",
        "        x = x.permute(0, 2, 3, 1, 4).contiguous() \\\n",
        "             .view(nbatches, nspace, ntime, self.nheads * self.d_k)\n",
        "        return self.linears[-1](x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nyalzInQvUUb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "import xarray as xr\n",
        "from pathlib import Path\n",
        "import random\n",
        "import torch\n",
        "\n",
        "\n",
        "def prepare_inputs_targets(len_time, input_gap, input_length, pred_shift, pred_length, samples_gap):\n",
        "    # input_gap=1: time gaps between two consecutive input frames\n",
        "    # input_length=12: the number of input frames\n",
        "    # pred_shift=26: the lead_time of the last target to be predicted\n",
        "    # pred_length=26: the number of frames to be predicted\n",
        "    assert pred_shift >= pred_length\n",
        "    input_span = input_gap * (input_length - 1) + 1\n",
        "    pred_gap = pred_shift // pred_length\n",
        "    input_ind = np.arange(0, input_span, input_gap) #array de 0 a len_time, intervalos de 1\n",
        "    target_ind = np.arange(0, pred_shift, pred_gap) + input_span + pred_gap - 1 #otro array de tanto a tanto\n",
        "    ind = np.concatenate([input_ind, target_ind]).reshape(1, input_length + pred_length)\n",
        "    max_n_sample = len_time - (input_span+pred_shift-1)\n",
        "    ind = ind + np.arange(max_n_sample)[:, np.newaxis] @ np.ones((1, input_length+pred_length), dtype=int)\n",
        "    idx_inputs = ind[::samples_gap, :input_length]\n",
        "    idx_targets = ind[::samples_gap, input_length:]\n",
        "\n",
        "\n",
        "\n",
        "    return idx_inputs, idx_targets\n",
        "\n",
        "\n",
        "def fold(data, size=36, stride=12):\n",
        "    # inverse of unfold/sliding window operation\n",
        "    # only applicable to the case where size of sliding window is n times that of stride\n",
        "    # data (N, size, *)\n",
        "    # outdata (N_, *)\n",
        "    # N/size is the number/width of sliding blocks\n",
        "    assert size % stride == 0\n",
        "    times = size // stride\n",
        "    remain = (data.shape[0] - 1) % times\n",
        "    if remain > 0:\n",
        "        ls = list(data[::times]) + [data[-1, -(remain*stride):]]\n",
        "        outdata = np.concatenate(ls, axis=0)  # (36*(151//3+1)+remain*stride, *, 15)\n",
        "    else:\n",
        "        outdata = np.concatenate(data[::times], axis=0)  # (36*(151/3+1), *, 15)\n",
        "    assert outdata.shape[0] == size * ((data.shape[0]-1)//times+1) + remain * stride\n",
        "    return outdata\n",
        "\n",
        "\n",
        "def data_transform(data, num_years_per_model):\n",
        "    # data (N, 36, *)\n",
        "    # num_years_per_model: 151/140\n",
        "    length = data.shape[0]\n",
        "    assert length % num_years_per_model == 0\n",
        "    num_models = length // num_years_per_model\n",
        "    outdata = np.stack(np.split(data, length/num_years_per_model, axis=0), axis=-1)  # (151, 36, *, 15)\n",
        "    outdata = fold(outdata)\n",
        "    # check output data\n",
        "    assert outdata.shape[-1] == num_models\n",
        "    assert not np.any(np.isnan(outdata))\n",
        "    return outdata\n",
        "\n",
        "\n",
        "def read_raw_data(ds_dir, out_dir=None):\n",
        "    train_cmip = xr.open_dataset(Path(ds_dir) / 'CMIP_train.nc').transpose('year', 'month', 'lat', 'lon')\n",
        "    label_cmip = xr.open_dataset(Path(ds_dir) / 'CMIP_label.nc').transpose('year', 'month')\n",
        "\n",
        "    lon = train_cmip.lon.values\n",
        "    lon = lon[np.logical_and(lon>=95, lon<=330)]\n",
        "    train_cmip = train_cmip.sel(lon=lon)\n",
        "\n",
        "    cmip6sst = data_transform(train_cmip.sst.values[:2265], 151)\n",
        "    cmip5sst = data_transform(train_cmip.sst.values[2265:], 140)\n",
        "    cmip6nino = data_transform(label_cmip.nino.values[:2265], 151)\n",
        "    cmip5nino = data_transform(label_cmip.nino.values[2265:], 140)\n",
        "\n",
        "\n",
        "    assert len(cmip6sst.shape) == 4\n",
        "    assert len(cmip5sst.shape) == 4\n",
        "    assert len(cmip6nino.shape) == 2\n",
        "    assert len(cmip5nino.shape) == 2\n",
        "    if out_dir is not None:\n",
        "        ds_cmip6 = xr.Dataset({'sst': (['month', 'lat', 'lon', 'model'], cmip6sst),\n",
        "                               'nino': (['month', 'model'], cmip6nino)},\n",
        "                              coords={'month': np.repeat(np.arange(1, 13)[None], cmip6nino.shape[0] // 12, axis=0).flatten(),\n",
        "                                      'lat': train_cmip.lat.values, 'lon': train_cmip.lon.values,\n",
        "                                      'model': np.arange(15)+1})\n",
        "        ds_cmip6.to_netcdf(Path(out_dir) / 'cmip6.nc')\n",
        "        ds_cmip5 = xr.Dataset({'sst': (['month', 'lat', 'lon', 'model'], cmip5sst),\n",
        "                               'nino': (['month', 'model'], cmip5nino)},\n",
        "                              coords={'month': np.repeat(np.arange(1, 13)[None], cmip5nino.shape[0] // 12, axis=0).flatten(),\n",
        "                                      'lat': train_cmip.lat.values, 'lon': train_cmip.lon.values,\n",
        "                                      'model': np.arange(17)+1})\n",
        "        ds_cmip5.to_netcdf(Path(out_dir) / 'cmip5.nc')\n",
        "    train_cmip.close()\n",
        "    label_cmip.close()\n",
        "    return cmip6sst, cmip5sst, cmip6nino, cmip5nino\n",
        "\n",
        "\n",
        "def read_from_nc(ds_dir):\n",
        "    cmip6 = xr.open_dataset(Path(ds_dir) / 'cmip6.nc').transpose('month', 'lat', 'lon', 'model')\n",
        "    cmip5 = xr.open_dataset(Path(ds_dir) / 'cmip5.nc').transpose('month', 'lat', 'lon', 'model')\n",
        "    return cmip6.sst.values, cmip5.sst.values, cmip6.nino.values, cmip5.nino.values\n",
        "\n",
        "\n",
        "def score(y_pred, y_true, acc_weight):\n",
        "    # for pytorch\n",
        "    # acc_weight = np.array([1.5]*4 + [2]*7 + [3]*7 + [4]*6) * np.log(np.arange(24)+1)\n",
        "    # acc_weight = torch.from_numpy(acc_weight).to(device)\n",
        "    pred = y_pred - y_pred.mean(dim=0, keepdim=True)  # (N, 24)\n",
        "    true = y_true - y_true.mean(dim=0, keepdim=True)  # (N, 24)\n",
        "    cor = (pred * true).sum(dim=0) / (torch.sqrt(torch.sum(pred**2, dim=0) * torch.sum(true**2, dim=0)) + 1e-6)\n",
        "    acc = (acc_weight * cor).sum()\n",
        "    rmse = torch.mean((y_pred - y_true)**2, dim=0).sqrt().sum()\n",
        "    return 2/3. * acc - rmse\n",
        "\n",
        "\n",
        "def cat_over_last_dim(data):\n",
        "    return np.concatenate(np.moveaxis(data, -1, 0), axis=0)\n",
        "\n",
        "\n",
        "class cmip_dataset(Dataset):\n",
        "    def __init__(self, sst_cmip6, nino_cmip6, sst_cmip5, nino_cmip5, samples_gap):\n",
        "        super().__init__()\n",
        "        # cmip6 (N, *, 15) 15 models!\n",
        "        # cmip5 (N, *, 17) 17 models!\n",
        "        input_sst = []\n",
        "        target_sst = []\n",
        "        target_nino = []\n",
        "        if sst_cmip6 is not None:\n",
        "            assert len(sst_cmip6.shape) == 4\n",
        "            assert len(nino_cmip6.shape) == 2\n",
        "            idx_input_sst, idx_target_sst = prepare_inputs_targets(sst_cmip6.shape[0], input_gap=1, input_length=12,\n",
        "                                                                   pred_shift=26, pred_length=26, samples_gap=samples_gap)\n",
        "\n",
        "            input_sst.append(cat_over_last_dim(sst_cmip6[idx_input_sst]))  # (N, 12, lat, lon, <=15) > (15*N, *)\n",
        "            target_sst.append(cat_over_last_dim(sst_cmip6[idx_target_sst]))  # (N, 26, lat, lon, <=15)\n",
        "            target_nino.append(cat_over_last_dim(nino_cmip6[idx_target_sst[:, :24]]))  # (N, 24, <=15)\n",
        "        if sst_cmip5 is not None:\n",
        "            assert len(sst_cmip5.shape) == 4\n",
        "            assert len(nino_cmip5.shape) == 2\n",
        "            idx_input_sst, idx_target_sst = prepare_inputs_targets(sst_cmip5.shape[0], input_gap=1, input_length=12,\n",
        "                                                                   pred_shift=26, pred_length=26, samples_gap=samples_gap)\n",
        "            input_sst.append(cat_over_last_dim(sst_cmip5[idx_input_sst]))  # (N, 12, lat, lon, <=17) > (17*N, *)\n",
        "            target_sst.append(cat_over_last_dim(sst_cmip5[idx_target_sst]))  # (N, 26, lat, lon, <=17)\n",
        "            target_nino.append(cat_over_last_dim(nino_cmip5[idx_target_sst[:, :24]]))  # (N, 24, <=17)\n",
        "\n",
        "        self.input_sst = np.concatenate(input_sst, axis=0)[:, :, None]  # (N, 12, 1, lat, lon)\n",
        "        self.target_sst = np.concatenate(target_sst, axis=0)  # (N, 26, 1, lat, lon)\n",
        "        self.target_nino = np.concatenate(target_nino, axis=0)  # (N, 24)\n",
        "        assert self.input_sst.shape[0] == self.target_sst.shape[0] == self.target_nino.shape[0]\n",
        "        assert self.input_sst.shape[1] == 12\n",
        "        assert self.target_sst.shape[1] == 26\n",
        "        assert self.target_nino.shape[1] == 24\n",
        "\n",
        "    def GetDataShape(self):\n",
        "        return {'sst input': self.input_sst.shape,\n",
        "                'sst target': self.target_sst.shape,\n",
        "                'nino target': self.target_nino.shape}\n",
        "\n",
        "    def __len__(self,):\n",
        "        return self.input_sst.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_sst[idx], self.target_sst[idx], self.target_nino[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1TEE-UysvUXS"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlVFQMzIveNR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "976a4fc5-25ef-4930-b244-ca7e78eaf113"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'device': device(type='cuda', index=0), 'batch_size_test': 100, 'batch_size': 8, 'weight_decay': 0.0, 'display_interval': 120, 'num_epochs': 100, 'early_stopping': True, 'patience': 5, 'gradient_clipping': True, 'clipping_threshold': 1.0, 'warmup': 3000, 'input_dim': 1, 'output_dim': 1, 'input_length': 12, 'output_length': 26, 'input_gap': 1, 'pred_shift': 24, 'd_model': 256, 'patch_size': (2, 3), 'emb_spatial_size': 192, 'nheads': 4, 'dim_feedforward': 512, 'dropout': 0.05, 'num_encoder_layers': 2, 'num_decoder_layers': 2, 'ssr_decay_rate': 0.0003}\n",
            "\n",
            "reading data\n",
            "processing training set\n",
            "{'sst input': (10410, 12, 1, 24, 48), 'sst target': (10410, 26, 24, 48), 'nino target': (10410, 24)}\n",
            "processing eval set\n",
            "{'sst input': (1667, 12, 1, 24, 48), 'sst target': (1667, 26, 24, 48), 'nino target': (1667, 24)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-8ead8d002574>:195: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(path)\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import pickle\n",
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "class NoamOpt:\n",
        "    \"\"\"\n",
        "    learning rate warmup and decay\n",
        "    \"\"\"\n",
        "    def __init__(self, model_size, factor, warmup, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "        self._step = 0\n",
        "        self.warmup = warmup\n",
        "        self.factor = factor\n",
        "        self.model_size = model_size\n",
        "        self._rate = 0\n",
        "\n",
        "    def step(self):\n",
        "        self._step += 1\n",
        "        rate = self.rate()\n",
        "        for p in self.optimizer.param_groups:\n",
        "            p['lr'] = rate\n",
        "        self._rate = rate\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def rate(self, step=None):\n",
        "        if step is None:\n",
        "            step = self._step\n",
        "        return self.factor * \\\n",
        "            (self.model_size ** (-0.5) * min(step ** (-0.5), step * self.warmup ** (-1.5)))\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, configs):\n",
        "        self.configs = configs\n",
        "        self.device = configs.device\n",
        "        torch.manual_seed(5)\n",
        "        self.network = SpaceTimeTransformer(configs).to(configs.device)\n",
        "        adam = torch.optim.Adam(self.network.parameters(), lr=0, weight_decay=configs.weight_decay)\n",
        "        factor = math.sqrt(configs.d_model*configs.warmup)*0.0014\n",
        "        self.opt = NoamOpt(configs.d_model, factor, warmup=configs.warmup, optimizer=adam)\n",
        "        self.weight = torch.from_numpy(np.array([1.5]*4 + [2]*7 + [3]*7 + [4]*6) * np.log(np.arange(24)+1)).to(configs.device)\n",
        "\n",
        "    def score(self, y_pred, y_true):\n",
        "        # compute for nino_pred, nino_true\n",
        "        with torch.no_grad():\n",
        "            sc = score(y_pred, y_true, self.weight)\n",
        "        return sc.item()\n",
        "\n",
        "    def loss_sst(self, y_pred, y_true):\n",
        "        # y_pred/y_true (N, 26, 24, 48)\n",
        "        rmse = torch.mean((y_pred - y_true)**2, dim=[2, 3])\n",
        "        rmse = torch.sum(rmse.sqrt().mean(dim=0))\n",
        "        return rmse\n",
        "\n",
        "    def loss_nino(self, y_pred, y_true):\n",
        "        with torch.no_grad():\n",
        "            rmse = torch.sqrt(torch.mean((y_pred - y_true)**2, dim=0)) * self.weight\n",
        "        return rmse.sum()\n",
        "\n",
        "    def train_once(self, input_sst, sst_true, nino_true, ssr_ratio):\n",
        "        sst_pred, nino_pred = self.network(src=input_sst.float().to(self.device),\n",
        "                                           tgt=sst_true[:, :, None].float().to(self.device),\n",
        "                                           train=True, ssr_ratio=ssr_ratio)\n",
        "        self.opt.optimizer.zero_grad()\n",
        "        loss_sst = self.loss_sst(sst_pred, sst_true.float().to(self.device))\n",
        "        loss_nino = self.loss_nino(nino_pred, nino_true.float().to(self.device))\n",
        "        loss_sst.backward()\n",
        "        if configs.gradient_clipping:\n",
        "            nn.utils.clip_grad_norm_(self.network.parameters(), configs.clipping_threshold)\n",
        "        self.opt.step()\n",
        "        return loss_sst.item(), loss_nino.item(), nino_pred\n",
        "\n",
        "    def test(self, dataloader_test):\n",
        "        nino_pred = []\n",
        "        sst_pred = []\n",
        "        with torch.no_grad():\n",
        "            for input_sst, sst_true, nino_true in dataloader_test:\n",
        "                sst, nino = self.network(src=input_sst.float().to(self.device),\n",
        "                                         tgt=None, train=False)\n",
        "                nino_pred.append(nino)\n",
        "                sst_pred.append(sst)\n",
        "\n",
        "        return torch.cat(sst_pred, dim=0), torch.cat(nino_pred, dim=0)\n",
        "\n",
        "\n",
        "\n",
        "    def infer(self, dataset, dataloader):\n",
        "        self.network.eval()\n",
        "        with torch.no_grad():\n",
        "            sst_pred, nino_pred = self.test(dataloader)\n",
        "            nino_true = torch.from_numpy(dataset.target_nino).float().to(self.device)\n",
        "            sst_true = torch.from_numpy(dataset.target_sst).float().to(self.device)\n",
        "            sc = self.score(nino_pred, nino_true)\n",
        "            loss_sst = self.loss_sst(sst_pred, sst_true).item()\n",
        "            loss_nino = self.loss_nino(nino_pred, nino_true).item()\n",
        "        return loss_sst, loss_nino, sc\n",
        "\n",
        "\n",
        "    def scoreCommon(self, y_pred, y_true):\n",
        "        # Standardize y_pred and y_true by subtracting their respective means\n",
        "        pred = y_pred - y_pred.mean(dim=0, keepdim=True)  # (N, 24)\n",
        "        true = y_true - y_true.mean(dim=0, keepdim=True)  # (N, 24)\n",
        "        # Compute correlation\n",
        "        cor = (pred * true).sum(dim=0) / (torch.sqrt(torch.sum(pred**2, dim=0) * torch.sum(true**2, dim=0)) + 1e-6)\n",
        "        cor = cor.sum() / 24  # Average correlation\n",
        "        # Compute RMSE\n",
        "        rmse = torch.mean((y_pred - y_true)**2, dim=0).sqrt()\n",
        "        return rmse, cor,true\n",
        "\n",
        "\n",
        "    def inferCommon(self, dataset, dataloader):\n",
        "        self.network.eval()  # Ensure evaluation mode\n",
        "        with torch.no_grad():\n",
        "            # Get predictions\n",
        "            sst_pred, nino_pred = self.test(dataloader)\n",
        "            # Convert dataset targets to tensors\n",
        "            nino_true = torch.from_numpy(dataset.target_nino).float().to(self.device)\n",
        "            acc_weight = self.weight  # Use pre-initialized weight\n",
        "            # Calculate RMSE and correlation\n",
        "            rmse, cor, true = self.scoreCommon(nino_pred, nino_true)\n",
        "        return rmse, cor,true\n",
        "\n",
        "\n",
        "    def train(self, dataset_train, dataset_eval, chk_path):\n",
        "        torch.manual_seed(0)\n",
        "        print('loading train dataloader')\n",
        "        dataloader_train = DataLoader(dataset_train, batch_size=self.configs.batch_size, shuffle=True)\n",
        "        print('loading eval dataloader')\n",
        "        dataloader_eval = DataLoader(dataset_eval, batch_size=self.configs.batch_size_test, shuffle=False)\n",
        "\n",
        "        count = 0\n",
        "        best = - math.inf\n",
        "        ssr_ratio = 1\n",
        "        for i in range(self.configs.num_epochs):\n",
        "            print('\\nepoch: {0}'.format(i+1))\n",
        "            # train\n",
        "            self.network.train()\n",
        "\n",
        "            start_time = time.time()\n",
        "            for j, (input_sst, sst_true, nino_true) in enumerate(dataloader_train):\n",
        "                if ssr_ratio > 0:\n",
        "                    ssr_ratio = max(ssr_ratio - self.configs.ssr_decay_rate, 0)\n",
        "                loss_sst, loss_nino, nino_pred = self.train_once(input_sst, sst_true, nino_true, ssr_ratio)  # y_pred for one batch\n",
        "\n",
        "                if j % self.configs.display_interval == 0:\n",
        "\n",
        "                    batch_time = time.time() - start_time\n",
        "\n",
        "                    start_time = time.time()\n",
        "                    sc = self.score(nino_pred, nino_true.float().to(self.device))\n",
        "                    print('batch training loss: {:.2f}, {:.2f}, score: {:.4f}, ssr: {:.5f}, lr: {:.5f}, time: {:.6f}'.format(loss_sst, loss_nino, sc, ssr_ratio, self.opt.rate(),batch_time))\n",
        "\n",
        "                # increase the number of evaluations in order not to miss the optimal point\n",
        "                # which is feasible because of the less training time of timesformer\n",
        "                if (i+1 >= 9) and (j+1)%300 == 0:\n",
        "                    _, _, sc_eval = self.infer(dataset=dataset_eval, dataloader=dataloader_eval)\n",
        "                    print('epoch eval loss: sc: {:.4f}'.format(sc_eval))\n",
        "                    if sc_eval > best:\n",
        "                        self.save_model(chk_path)\n",
        "                        best = sc_eval\n",
        "                        count = 0\n",
        "\n",
        "            # evaluation\n",
        "            loss_sst_eval, loss_nino_eval, sc_eval = self.infer(dataset=dataset_eval, dataloader=dataloader_eval)\n",
        "            print('epoch eval loss:\\nsst: {:.2f}, nino: {:.2f}, sc: {:.4f}'.format(loss_sst_eval, loss_nino_eval, sc_eval))\n",
        "\n",
        "            print(\"Current time:\", datetime.now())\n",
        "            if sc_eval <= best:\n",
        "                count += 1\n",
        "                print('eval score is not improved for {} epoch'.format(count))\n",
        "            else:\n",
        "                count = 0\n",
        "                print('eval score is improved from {:.5f} to {:.5f}, saving model'.format(best, sc_eval))\n",
        "                self.save_model(chk_path)\n",
        "                best = sc_eval\n",
        "\n",
        "            if count == self.configs.patience:\n",
        "                print('early stopping reached, best score is {:5f}'.format(best))\n",
        "                break\n",
        "\n",
        "\n",
        "    def save_configs(self, config_path):\n",
        "        with open(config_path, 'wb') as path:\n",
        "            pickle.dump(self.configs, path)\n",
        "\n",
        "    def save_model(self, path):\n",
        "        torch.save({'net': self.network.state_dict(),\n",
        "                    'optimizer': self.opt.optimizer.state_dict()}, path)\n",
        "    def load_model(self, path):\n",
        "        checkpoint = torch.load(path)\n",
        "        self.network.load_state_dict(checkpoint['net'])\n",
        "        self.opt.optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "\n",
        "def prepare_data(ds_dir):\n",
        "    # train/eval/test split\n",
        "    cmip6sst, cmip5sst, cmip6nino, cmip5nino = read_raw_data(ds_dir)\n",
        "    sst_train = [cmip6sst, cmip5sst[..., :-2]]\n",
        "    nino_train = [cmip6nino, cmip5nino[..., :-2]]\n",
        "    sst_eval = [cmip5sst[..., -2:-1]]\n",
        "    nino_eval = [cmip5nino[..., -2:-1]]\n",
        "    sst_test = [cmip5sst[..., -1:]]\n",
        "    nino_test = [cmip5nino[..., -1:]]\n",
        "    return sst_train, nino_train, sst_eval, nino_eval, sst_test, nino_test\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    print(configs.__dict__)\n",
        "\n",
        "    print('\\nreading data')\n",
        "    sst_train, nino_train, sst_eval, nino_eval, sst_test, nino_test = prepare_data('/content/gdrive/MyDrive/Tesis_Tito/codigo/earthformerDependecies/datasets/icar_enso_2021/enso_round1_train_20210201')\n",
        "\n",
        "    print('processing training set')\n",
        "    dataset_train = cmip_dataset(sst_train[0], nino_train[0], sst_train[1], nino_train[1], samples_gap=5)\n",
        "    print(dataset_train.GetDataShape())\n",
        "    del sst_train\n",
        "    del nino_train\n",
        "    print('processing eval set')\n",
        "    dataset_eval = cmip_dataset(sst_cmip6=None, nino_cmip6=None,\n",
        "                                sst_cmip5=sst_eval[0], nino_cmip5=nino_eval[0], samples_gap=1)\n",
        "    print(dataset_eval.GetDataShape())\n",
        "    del sst_eval\n",
        "    del nino_eval\n",
        "    trainer = Trainer(configs)\n",
        "    trainer.load_model('/content/gdrive/MyDrive/Tesis_Tito/codigo/sc 31 e21 e2 cR d 0.5.chk')\n",
        "    #trainer.save_configs('/content/gdrive/MyDrive/Tesis_Tito/codigo/config_train.pkl')\n",
        "    #trainer.train(dataset_train, dataset_eval, '/content/gdrive/MyDrive/Tesis_Tito/codigo/checkpoint.chk')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader_eval = DataLoader(dataset_eval, batch_size=configs.batch_size_test, shuffle=False)\n",
        "\n",
        "rmse, cor, true = trainer.inferCommon(dataset=dataset_eval, dataloader=dataloader_eval)\n",
        "print(rmse)\n",
        "print(cor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkIERxd3aiSQ",
        "outputId": "066e2096-5921-42c6-f304-1a980624c77b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.2155, 0.2921, 0.3554, 0.4103, 0.4594, 0.5042, 0.5452, 0.5822, 0.6145,\n",
            "        0.6418, 0.6636, 0.6799, 0.6905, 0.6959, 0.6972, 0.6958, 0.6932, 0.6910,\n",
            "        0.6901, 0.6909, 0.6929, 0.6957, 0.6992, 0.7036], device='cuda:0')\n",
            "tensor(0.5322, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "GUo_a428uwuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLtQNeMspO2J"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming your tensor is named `tensor`\n",
        "numpy_array = true.cpu().numpy()  # Move to CPU and convert to NumPy\n",
        "df = pd.DataFrame(numpy_array)\n",
        "\n",
        "# Optional: Add custom column names or indexes if needed\n",
        "df.columns = [f\"mes {i+1}\" for i in range(df.shape[1])]  # Example column names\n",
        "df.head(20)\n"
      ],
      "metadata": {
        "id": "hZSpezlbib4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.shape)"
      ],
      "metadata": {
        "id": "XrHag3wQZWiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6LH4Gla2WJc"
      },
      "source": [
        "19 10.6 con rec dsik 35.4 executado x segunda vez\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "0.01 weigth da negativo\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "0.0001 wigth da normalito\n",
        "\n",
        "ssr rate alta negativo\n",
        "\n",
        "\n",
        "---\n",
        "e 13 sc 30.7 ???\n",
        "\n",
        "e21 sc 31.8893 e2 d 2 con R d 0.5\n",
        "\n",
        "e14 sc 28.005715 e 2 d 2 con R d 0.1\n",
        "\n",
        "e7 sc 23.26 e 2 d 2 con R d 0.15"
      ],
      "metadata": {
        "id": "vooSa_Qqjk7i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "epoch: 13\n",
        "batch training loss: 10.00, 24.68, score: 102.7091, ssr: 0.00000, lr: 0.00059\n",
        "epoch eval loss:\n",
        "sst: 11.94, nino: 115.34, sc: 25.7620\n",
        "Current time: 2024-09-22 00:23:54.201798\n",
        "eval score is not improved for 4 epoch\n",
        "early stopping reached, best score is 30.743836\n"
      ],
      "metadata": {
        "id": "9QosALkSjlFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "\n",
        "# Assume you have a DataLoader, model, and optimizer defined\n",
        "def estimate_training_time(model, dataloader, optimizer, device=\"cuda\"):\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    # Get a single batch from the dataloader\n",
        "    inputs, targets = next(iter(dataloader))\n",
        "    inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "    # Measure time for a single forward and backward pass\n",
        "\n",
        "    outputs = model(inputs)            # Forward pass\n",
        "    loss = torch.nn.functional.mse_loss(outputs, targets)  # Example loss\n",
        "    loss.backward()                    # Backward pass\n",
        "    optimizer.step()                   # Update weights\n",
        "    optimizer.zero_grad()              # Reset gradients\n",
        "\n",
        "\n",
        "    # Estimate total time\n",
        "    num_batches = len(dataloader)\n",
        "    total_time = batch_time * num_batches * epochs  # Adjust epochs as needed\n",
        "    return batch_time, total_time\n",
        "\n",
        "# Example usage\n",
        "epochs = 10\n",
        "batch_time, total_time = estimate_training_time(model, dataloader, optimizer)\n",
        "print(f\"Time per batch: {batch_time:.4f} seconds\")\n",
        "print(f\"Estimated total training time: {total_time / 60:.2f} minutes\")\n"
      ],
      "metadata": {
        "id": "PdUdhmuduCaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'/content/gdrive/MyDrive/Tesis_Tito/codigo/checkpoint.chk'"
      ],
      "metadata": {
        "id": "yEAemeKewp-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "1/0.000/1201"
      ],
      "metadata": {
        "id": "lZH5ZWe7a86u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Define the parameters\n",
        "base_score = 31.5\n",
        "base_lr = 0.0001\n",
        "\n",
        "# Learning rates (doubling and quartering)\n",
        "learning_rates = [0.00005,0.0002,0.0003,0.00045,0.0006]\n",
        "\n",
        "# Scores with a smooth trend for the plot\n",
        "scores = [1/0.00005/1200,1/0.0002/1200,1/0.0003/1200,1/0.00045/1200,1/0.0006/1200]\n",
        "\n",
        "# Set the style and palette for seaborn\n",
        "sns.set(style=\"whitegrid\", palette=\"muted\")\n",
        "\n",
        "# Create the plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot with seaborn style\n",
        "sns.lineplot(x=learning_rates, y=scores, marker=\"o\", linewidth=2.5, markersize=8)\n",
        "\n",
        "# Fill the area under the curve with a light color\n",
        "plt.fill_between(learning_rates, scores, color=\"skyblue\", alpha=0.3)\n",
        "plt.xscale('log')\n",
        "# Customize the x-axis to logarithmic scale\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('Learning Rate', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('Epoch', fontsize=14, fontweight='bold')\n",
        "plt.title('Primera Predicción Con Solo Dato Inicial', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Show the plot with tight layout\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "cVD85fBYuTLu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 601
        },
        "outputId": "153b5638-a2c0-4e31-ee17-0d241cd4ff38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xarray as xr\n",
        "\n",
        "# Assuming you already have sst_train loaded\n",
        "sst_train, nino_train, sst_eval, nino_eval, sst_test, nino_test = prepare_data('/content/gdrive/MyDrive/Tesis_Tito/codigo/earthformerDependecies/datasets/icar_enso_2021/enso_round1_train_20210201')\n",
        "\n",
        "sst_train = sst_train[0][0]\n",
        "# Print the shape to confirm it's (1836, 24, 48, 15)\n",
        "model_1 = sst_train[:, :, 0]\n",
        "model_2 = sst_train[:, :, 1]\n",
        "\n",
        "latitudes = np.arange(-55, 61, 5)  # Latitude from -55 to 60 with a gap of 5\n",
        "longitudes = np.arange(95, 331, 5)  # Longitude from 95 to 330 with a gap of 5\n",
        "# Create latitudes as row indices\n",
        "\n",
        "# Create DataFrames for each model\n",
        "df_model_1 = pd.DataFrame(model_1, index=latitudes, columns=longitudes)\n",
        "df_model_2 = pd.DataFrame(model_2, index=latitudes, columns=longitudes)\n",
        "\n",
        "# Display the tables\n",
        "\n",
        "df_model_1.head(100)\n",
        "\n",
        "\n",
        "\n",
        "# Clean up by deleting variables if needed\n",
        "del sst_train\n",
        "del nino_train\n",
        "del sst_eval\n",
        "\n",
        "df_model_2.round(2).head(-5)\n"
      ],
      "metadata": {
        "id": "EcUB-B0FPtvn"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOFw/OGquNkfUEF7ajyHyzC"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}